---
title: 'Homework: iterative method'
output:
  html_document:
    df_print: paged
---

```{r setup, include = FALSE}
source(file.path("../..", "R", "util.R"))
required_packages <- c('RSpectra','tidyverse','testit')
install_and_load_packages(required_packages)
```

# Problem 1
Auto regressive processes can be viewed as a discrete analog of Ornsteinâ€“Uhlenbeck process &mdash; which coincides with Gaussian process based on an exponential covariance matrix &mdash; and hence is an example of Gaussian Markov random fields.
For instance, stationary lag-1 auto-regressive process 
$$x_t = \phi x_{t - 1} + \sqrt{1 - \phi} \, \epsilon_t, 
  \quad x_0 \sim \mathcal{N}(0, 1), 
  \quad \epsilon_t \mathbin{\overset{\small \textrm{i.i.d.}}{\sim}} \mathcal{N}(0, 1)$$
has the _tri-diagonal_ precision matrix
$$\boldsymbol{\Sigma}^{-1} = \frac{1}{1 - \phi^2} 
  \begin{bmatrix} 
  1 & -\phi & 0 & & & \ldots & 0 \\
  -\phi & 1 + \phi^2 & -\phi & 0 & & & \vdots \\
  0 & -\phi & 1 + \phi^2 & -\phi & 0 & & \\
    & & \ddots & \ddots & \ddots & & \\
    & & 0 & -\phi & 1 + \phi^2 & -\phi & 0 \\
  \vdots & &   & 0 & -\phi & 1 + \phi^2 & -\phi \\
  0 & \ldots &   &   & 0 & -\phi & 1\\
  \end{bmatrix}.$$
More generally, a lag-$k$ (non-stationary) auto-regressive process has a _banded_ precision matrix with bandwidth $k$.

Implement a fast matrix-vector $\boldsymbol{v} \to \boldsymbol{\Sigma}^{-1} \boldsymbol{v}$ operation, exploiting the structure of the AR-1 precision matrix.
Then use this function to find the top 10 principal components of $\boldsymbol{\Sigma}$ (not $\boldsymbol{\Sigma}^{-1}$) via Lanczos algorithm provided via `RSpectra::eigs_sym`.

<font color="blue">
We use `eigs_sym` to calculate 10 **smallest eigenvalues** of $\boldsymbol{\Sigma}^{-1}$ and corresponding eigenvectors by specifying `which = SM`. We then calculated the inverse of those 10 smallest eigenvalues to derive the largest 10 eigenvalues of $\boldsymbol{\Sigma}$. The eigenvectors remain the same. 
</font>

```{r}
ar_length <- 4096
auto_corr <- .9 # Corresponds to `\phi` above

ar_precision_matvec <- function(v, auto_corr) {
  # Fill in
  n = length(v)
  
  vt1 = v[1]- auto_corr * v[2]
  vtn= - auto_corr * v[n-1] + v[n]
  vt = sapply(1:(n-2), function(i){
    - auto_corr * v[i] + (1+ auto_corr^2) * v[i+1] - auto_corr * v[i+2]
  })
  v = c(vt1,vt,vtn)/(1-auto_corr^2)
  return(v)
}

ar_eig <- eigs_sym(
  ar_precision_matvec, args = auto_corr,
  k = 10,which = 'SM',
  # Fill in
  opts = list(
    ncv = 100, # Spectrum distribution of AR-1 process is not very spread out on the extreme ends and is actually a hard case for Lanczos. So it helps to have more Lanczos vectors than the default for faster convergence.
    maxitr = 10^3, # Cap it just in case
    retvec = TRUE # More efficient to do without eigenvectors when not needed
  ),
  n = ar_length
)
```

Now, directly compute the eigen decomposition of $\boldsymbol{\Sigma}$ (not $\boldsymbol{\Sigma}^{-1}$) and compare its output with the principal components and associated variances found via Lancsoz algorithm.

<font color="blue">
We can use mathematical induction to derive the covairance matrix for AR-1 process.
$$
Cov(X_s,X_t) = \begin{cases}
1 & \text{for} \ s =t \\
\phi^{|s-t|} & \text{for} \ s \neq t
\end{cases}
$$

</font>

```{r}
# Fill in
eigen_decomp_AR1_cov = function(ar_length,auto_corr, k){
  Sigma = lapply(1:ar_length, function(i){
    auto_corr^abs(i-c(1:ar_length))
  })
  Sigma = do.call(rbind,Sigma)
  eigen_decomp = eigen(Sigma)
  eigen_vec = eigen_decomp$vectors[,1:k]
  eigen_val <- eigen_decomp$values[1:k]
  return(list(values = eigen_val, vectors = eigen_vec))
}
ar_eig_decomp = eigen_decomp_AR1_cov(ar_length = ar_length, auto_corr = auto_corr, k = 10) 
```

<font color="blue">
We then use the function `are_all_close` to verify if we obtain the same eigenvalues and eigenvetors via 
Lancsoz algorithm and direct eigen decomposition. For the output of Lancsoz algorithm, we need to take the inverse of eigenvalues and reorder the eigenvectors before comparing the results with eigen decomposition.
</font>

```{r}
ar_eig_val = sort(1/ar_eig$values, decreasing = T)
ar_eig_vec = ar_eig$vectors[,order(1/ar_eig$values,decreasing = T)]

assert("Lancsoz algorithm results are consistent with the eigen decomposition results", {
  are_all_close(ar_eig_val, ar_eig_decomp$values)
  are_all_close(abs(ar_eig_vec), abs(ar_eig_decomp$vectors),rel_tol = 2e-2) 
  ## in case the vectors are in the opposite direction 
  ## in case some entries have higher relative error 
})
```


**Remark:** 
For banded matrices, there actually are even more efficient approaches.
To get a sense of special routines available for banded matrices, you can take a look at `*_banded` functions in [SciPy's linear algebra routines](https://docs.scipy.org/doc/scipy/reference/linalg.html).
Even those functions represent only a subset of available numerical linear algebra techniques; see
[LAPACK documentation for SVD](https://www.netlib.org/lapack/lug/node32.html) for example.
